{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wX6Jx5W5PEHY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.layers import LSTM, Dense, Masking\n",
        "\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdyqLJaTPKDh",
        "outputId": "8dd842be-c197-4486-c11f-848e0ac2e0ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eoRRWGA2PEHc"
      },
      "outputs": [],
      "source": [
        "body_parts = [\n",
        "            'mouth', 'eye', 'skull', 'upper tail bone', 'lower tail bone',\n",
        "            'upper tail', 'lower tail', 'pectoral fin', 'anal fin start',\n",
        "            'anal fin mid', 'dorsal fin_base', 'dorsal fin_tip', 'stomach', 'middle'\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rWHjYulPEHd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Gg1X6w4GPEHe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def prepare_individuals(data_numeric, target_length=141, body_parts=None, num_individuals=8):\n",
        "    if body_parts is None:\n",
        "        body_parts = [\n",
        "            'mouth', 'eye', 'skull', 'upper tail bone', 'lower tail bone',\n",
        "            'upper tail', 'lower tail', 'pectoral fin', 'anal fin start',\n",
        "            'anal fin mid', 'dorsal fin_base', 'dorsal fin_tip', 'stomach', 'middle'\n",
        "        ]\n",
        "\n",
        "    def process_column(column, target_length):\n",
        "        result_array = np.zeros(target_length)\n",
        "        non_nan_indices = np.where(~column.isna())[0]\n",
        "        if len(non_nan_indices) > 1:\n",
        "            valid_values = column[non_nan_indices]\n",
        "            differences = np.diff(valid_values)\n",
        "            for i, diff in enumerate(differences):\n",
        "                result_array[non_nan_indices[i + 1]] = diff\n",
        "        return result_array\n",
        "\n",
        "    individual_features = {}\n",
        "\n",
        "    for individual in range(1, num_individuals + 1):\n",
        "        features_list = []\n",
        "\n",
        "        for idx, body_part in enumerate(body_parts):\n",
        "            if individual == 1 and idx == 0:\n",
        "                x_col_name = 'x'\n",
        "                y_col_name = 'y'\n",
        "            else:\n",
        "                x_col_name = f'x.{(individual - 1) * len(body_parts) + idx}'\n",
        "                y_col_name = f'y.{(individual - 1) * len(body_parts) + idx}'\n",
        "\n",
        "            if x_col_name in data_numeric.columns and y_col_name in data_numeric.columns:\n",
        "                delta_x = process_column(data_numeric[x_col_name], target_length)\n",
        "                delta_y = process_column(data_numeric[y_col_name], target_length)\n",
        "\n",
        "                if len(delta_x) > 0 and len(delta_y) > 0:\n",
        "                    speed = np.insert(np.sqrt(delta_x**2 + delta_y**2), 0, 0)\n",
        "                    direction = np.insert(np.arctan2(delta_y, delta_x), 0, 0)\n",
        "                    direction_degrees = np.degrees(direction)\n",
        "\n",
        "                    features_list.append(speed)\n",
        "                    features_list.append(direction_degrees)\n",
        "\n",
        "        if features_list:\n",
        "            individual_features[f'individual{individual}'] = pd.DataFrame(features_list).transpose()\n",
        "\n",
        "    return individual_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the model on data with 7 individual fishes, each having 14 keypoints for 141 images and 3 different species. The video used is from the FishTrac dataset 'V1_Leleiwi_26June19_17.mp4'. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c11bsrapPEHf",
        "outputId": "da69b742-7b79-47e5-d9f0-eca70d4bb93b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(141, 227)\n",
            "individual1: 28 columns\n",
            "individual2: 28 columns\n",
            "individual3: 28 columns\n",
            "individual4: 28 columns\n",
            "individual5: 28 columns\n",
            "individual6: 28 columns\n",
            "individual7: 28 columns\n"
          ]
        }
      ],
      "source": [
        "# data_numeric = pd.read_csv('/content/drive/MyDrive/CollectedData_jaime.csv', skiprows=3)\n",
        "data_numeric = pd.read_csv('TrainDataFishVideo.csv', skiprows=3)\n",
        "print(data_numeric.shape)\n",
        "\n",
        "data = prepare_individuals(data_numeric, target_length=141, body_parts=body_parts, num_individuals=8)\n",
        "\n",
        "if 'individual8' in data:\n",
        "    del data['individual8']\n",
        "\n",
        "for key in data.keys():\n",
        "    print(f\"{key}: {len(data[key].columns)} columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kQY535KPPEHh"
      },
      "outputs": [],
      "source": [
        "# Asignar las etiquetas a los individuos\n",
        "jaime_labels = [0, 0, 2, 2, 1, 2, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA5eR5-lWTBD"
      },
      "source": [
        "# Oversample class 1 and 0\n",
        "Since we have only 8 individual fish examples for training for three different species then it is important that our data is balanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlgJjVDRUSym",
        "outputId": "7572c898-cf43-470c-a44a-3436a625335c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0787 - accuracy: 0.3333 - val_loss: 0.9702 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.9569 - accuracy: 0.5000 - val_loss: 0.9416 - val_accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.8419 - accuracy: 1.0000 - val_loss: 0.9087 - val_accuracy: 0.5000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.7295 - accuracy: 1.0000 - val_loss: 0.8739 - val_accuracy: 0.5000\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.6186 - accuracy: 1.0000 - val_loss: 0.8411 - val_accuracy: 0.5000\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.5231 - accuracy: 1.0000 - val_loss: 0.8136 - val_accuracy: 0.5000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.4567 - accuracy: 1.0000 - val_loss: 0.7921 - val_accuracy: 0.5000\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.4072 - accuracy: 1.0000 - val_loss: 0.7652 - val_accuracy: 0.5000\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.3695 - accuracy: 1.0000 - val_loss: 0.7088 - val_accuracy: 0.5000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.3330 - accuracy: 1.0000 - val_loss: 0.6545 - val_accuracy: 0.5000\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Asignar las etiquetas a los individuos\n",
        "jaime_labels = [0, 0, 2, 2, 1, 2, 1]\n",
        "\n",
        "# Asumiendo que jaime_data es un diccionario de DataFrames y jaime_labels ya está definido\n",
        "all_data = []\n",
        "all_labels = []\n",
        "\n",
        "for key, df in data.items():\n",
        "    # Convertir el DataFrame a un array 3D (samples, time steps, features)\n",
        "    individual_data = np.expand_dims(df.values, axis=0)\n",
        "    all_data.append(individual_data)\n",
        "    all_labels.append(jaime_labels[len(all_data)-1])  # Asegúrate de que jaime_labels esté en el orden correcto\n",
        "\n",
        "# Convertir listas a arrays de NumPy\n",
        "all_data = np.concatenate(all_data, axis=0)\n",
        "all_labels = to_categorical(all_labels, num_classes=3)  # Convertir etiquetas a categóricas\n",
        "# X_resampled and y_resampled are now the resampled feature set and labels, respectively\n",
        "\n",
        "# Dividir los datos y las etiquetas en conjuntos de entrenamiento y validación (80% - 20%)\n",
        "train_data, validation_data, train_labels, validation_labels = train_test_split(\n",
        "    all_data, all_labels, test_size=0.2, random_state=42)\n",
        "# Flatten the time steps and features into a single dimension\n",
        "# Flatten the time series data into 2D\n",
        "nsamples, nx, ny = train_data.shape\n",
        "train_data_2d = train_data.reshape((nsamples, nx*ny))\n",
        "\n",
        "# Perform oversampling on 2D data\n",
        "ros = RandomOverSampler(random_state=0)\n",
        "X_resampled_2d, y_resampled = ros.fit_resample(train_data_2d, train_labels)\n",
        "\n",
        "# Reshape the data back to 3D\n",
        "X_resampled = X_resampled_2d.reshape((-1, nx, ny))\n",
        "\n",
        "# Shuffle the dataset to ensure random distribution after resampling (optional but recommended)\n",
        "X_resampled, y_resampled = shuffle(X_resampled, y_resampled, random_state=0)\n",
        "\n",
        "y_resampled_encoded = to_categorical(y_resampled, num_classes=3)\n",
        "# Now, `X_resampled` and `y_resampled` have a balanced class distribution\n",
        "# You can then proceed to train your LSTM model with this resampled data\n",
        "validation_labels_encoded = to_categorical(validation_labels, num_classes=3)\n",
        "\n",
        "\n",
        "model1 = Sequential([\n",
        "    LSTM(50, input_shape=(train_data.shape[1], train_data.shape[2])),  # 50 unidades LSTM\n",
        "    Dense(3, activation='softmax')  # Capa de salida para 3 clases\n",
        "])\n",
        "model1.add(Masking(mask_value=0., input_shape=(141, 56)))  # Assuming each body part has 2 features: speed and direction\n",
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model1.fit(X_resampled, y_resampled_encoded, epochs=10, validation_data=(validation_data, validation_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing the model on another video from the fishTrac dataset. This video contains 10 individuals and 57 frames. The video is '02_Oct_18_Vid-3.mp4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eVu7aMKPEHj",
        "outputId": "0402ab07-d5fb-42f9-92c9-052fa771b774"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(56, 311)\n",
            "individual1: 28 columns\n",
            "individual2: 28 columns\n",
            "individual3: 28 columns\n",
            "individual4: 28 columns\n",
            "individual5: 28 columns\n",
            "individual6: 28 columns\n",
            "individual7: 28 columns\n",
            "individual8: 28 columns\n",
            "individual9: 28 columns\n",
            "individual10: 28 columns\n"
          ]
        }
      ],
      "source": [
        "# katia_data_numeric = pd.read_csv('/content/drive/MyDrive/CollectedData_katia.csv', skiprows=3)\n",
        "test_data_numeric = pd.read_csv('TrainDataFishVideo.csv', skiprows=3)\n",
        "\n",
        "print(test_data_numeric.shape)\n",
        "\n",
        "test_data = prepare_individuals(test_data_numeric, target_length=56, body_parts=body_parts, num_individuals=10)\n",
        "\n",
        "if 'individual11' in test_data:\n",
        "    del test_data['individual11']\n",
        "\n",
        "for key in test_data.keys():\n",
        "    print(f\"{key}: {len(test_data[key].columns)} columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Prepare test data with padding\n",
        "test_data_padded = []\n",
        "\n",
        "for key, df in test_data.items():\n",
        "    # Convert the DataFrame to a 3D array (samples, time steps, features)\n",
        "    individual_data = np.expand_dims(df.values, axis=0)\n",
        "    # Pad sequences to match the model's expected input shape (142 time steps)\n",
        "    individual_data_padded = pad_sequences(individual_data, maxlen=142, dtype='float32', padding='post', truncating='post', value=0.0)\n",
        "    test_data_padded.append(individual_data_padded)\n",
        "\n",
        "# Convert list to a NumPy array\n",
        "test_data_padded = np.concatenate(test_data_padded, axis=0)\n",
        "\n",
        "# Make predictions with padded test data\n",
        "predictions = model1.predict(test_data_padded)\n",
        "print(predictions)\n",
        "# Optionally, convert predictions to labels\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMFUG3C4VqkE",
        "outputId": "2011189a-3f00-4c9b-f83e-f21f3cafead8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy: 40.00%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming you have a variable `true_labels` which contains the true class indices\n",
        "true_labels = np.array([0, 1, 0, 2, 1, 3, 3,1,1,3])\n",
        "\n",
        "\n",
        "# Now you have the predicted class indices, you can compare them with the true labels\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "# Print out the accuracy\n",
        "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTRv_GSOVmQz"
      },
      "outputs": [],
      "source": [
        "true_labels = [0, 1, 0, 2, 1, 3, 3,1,1]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gpu0",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
